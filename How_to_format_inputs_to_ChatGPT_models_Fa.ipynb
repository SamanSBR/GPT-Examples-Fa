{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fjN7mvOWFt0"
      },
      "source": [
        "# نحوه قالب‌بندی ورودی‌ها برای مدل‌های ChatGPT\n",
        "\n",
        "این نوت‌بوک توضیح می‌دهد که چگونه می‌توان ورودی‌های متنی را برای مدل‌های ChatGPT قالب‌بندی کرد. این مدل‌ها شامل GPT-3.5 و GPT-4 هستند که از رابط چت‌محور استفاده می‌کنند.\n",
        "\n",
        "به‌جای ارسال یک رشته‌ی ساده به عنوان ورودی، این مدل‌ها انتظار دارند **لیستی از پیام‌ها** دریافت کنند. هر پیام یک دیکشنری با دو فیلد ضروری است:\n",
        "- `role` (که می‌تواند `system`، `user` یا `assistant` باشد)\n",
        "- `content` (محتوای پیام)\n",
        "\n",
        "مدل از این پیام‌ها به‌عنوان سابقه‌ی گفتگو استفاده می‌کند و خروجی آن، ادامه‌ی منطقی همان گفتگو خواهد بود.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hhw2qQCWFt2",
        "outputId": "c875923b-11bb-4dff-8a77-d4332736aede"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.93.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n"
          ]
        }
      ],
      "source": [
        "# if needed, install and/or upgrade to the latest version of the OpenAI Python library\n",
        "%pip install --upgrade openai\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG8-bgUnWFt2"
      },
      "source": [
        "## ۱. کتابخانه openai را وارد کنید"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GVZuTUYMWFt3"
      },
      "outputs": [],
      "source": [
        "# import the OpenAI Python library for calling the OpenAI API\n",
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lf58b1A3WFt3"
      },
      "source": [
        "## ۲. یک نمونه از فراخوانی API برای تولید پاسخ چت\n",
        "\n",
        "پارامترهای مورد نیاز برای فراخوانی API تولید پاسخ چت:\n",
        "\n",
        "**ضروری**\n",
        "- `model`: نام مدلی که می‌خواهید استفاده کنید (مثلاً: `gpt-3.5-turbo`, `gpt-4`, `gpt-3.5-turbo-16k-1106`)\n",
        "- `messages`: یک لیست از شیءهای پیام (message)، که هر کدام شامل دو فیلد ضروری هستند:\n",
        "  - `role`: نقش فرستنده پیام (می‌تواند یکی از `system`، `user`، `assistant` یا `tool` باشد)\n",
        "  - `content`: محتوای پیام (مثلاً: `برایم یک شعر زیبا بنویس`)\n",
        "\n",
        "هر پیام می‌تواند به‌صورت اختیاری شامل فیلد `name` نیز باشد که به فرستنده پیام یک نام می‌دهد، مثلاً: `example-user`، `Alice`، یا `BlackbeardBot`. توجه داشته باشید که نام‌ها نباید شامل فاصله (space) باشند.\n",
        "\n",
        "**اختیاری**\n",
        "- `frequency_penalty`: بر اساس تکرار واژه‌ها، احتمال تکرار آن‌ها را کاهش می‌دهد.\n",
        "- `logit_bias`: احتمال ظاهر شدن برخی توکن‌ها را با مقادیر بایاس تغییر می‌دهد.\n",
        "- `logprobs`: در صورت فعال بودن، احتمال لگاریتمی توکن‌های خروجی را برمی‌گرداند.\n",
        "- `top_logprobs`: تعداد توکن‌هایی که بیشترین احتمال را دارند در هر موقعیت مشخص می‌کند.\n",
        "- `max_tokens`: حداکثر تعداد توکن‌های تولیدشده در پاسخ چت را تعیین می‌کند.\n",
        "- `n`: تعداد پاسخ‌های جایگزین برای هر ورودی را مشخص می‌کند.\n",
        "- `presence_penalty`: احتمال ظاهر شدن توکن‌هایی را که قبلاً در متن آمده‌اند کاهش می‌دهد.\n",
        "- `response_format`: فرمت خروجی را مشخص می‌کند، مثلاً حالت JSON.\n",
        "- `seed`: با مقداردهی اولیه ثابت، تولید پاسخ تکرارشونده (deterministic) را ممکن می‌سازد.\n",
        "- `stop`: تا ۴ رشته را مشخص می‌کند که در صورت رسیدن به آن‌ها، مدل تولید پاسخ را متوقف کند.\n",
        "- `stream`: خروجی را به صورت بخش‌بخش (streaming) و زنده برمی‌گرداند.\n",
        "- `temperature`: میزان تصادفی بودن نمونه‌گیری را بین ۰ تا ۲ تعیین می‌کند.\n",
        "- `top_p`: از نمونه‌گیری هسته‌ای (nucleus sampling) استفاده می‌کند و فقط توکن‌هایی را در نظر می‌گیرد که مجموع احتمالشان به `top_p` برسد.\n",
        "- `tools`: لیستی از توابعی که مدل می‌تواند فراخوانی کند.\n",
        "- `tool_choice`: نحوه انتخاب تابع توسط مدل را کنترل می‌کند (none/auto/function).\n",
        "- `user`: شناسه منحصربه‌فرد کاربر نهایی، برای نظارت و جلوگیری از سوءاستفاده.\n",
        "\n",
        "از ژانویه ۲۰۲۴، همچنین می‌توانید به صورت اختیاری لیستی از `functions` ارائه دهید که به GPT اجازه می‌دهد خروجی JSON تولید کند تا به عنوان ورودی یک تابع استفاده شود. برای جزئیات بیشتر، به [مستندات رسمی](https://platform.openai.com/docs/guides/function-calling)، [مرجع API](https://platform.openai.com/docs/api-reference/chat) یا راهنمای Cookbook [نحوه فراخوانی توابع با مدل‌های چت](How_to_call_functions_with_chat_models.ipynb) مراجعه کنید.\n",
        "\n",
        "معمولاً یک مکالمه با یک پیام `system` آغاز می‌شود که نحوه رفتار مدل را تعیین می‌کند، سپس پیام‌های متناوب بین `user` و `assistant` ادامه پیدا می‌کند — اما پیروی از این ساختار الزامی نیست.\n",
        "\n",
        "بیایید یک نمونه فراخوانی واقعی API را ببینیم تا ببینیم فرمت چت در عمل چگونه کار می‌کند.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "kDRGQOeLWFt4",
        "outputId": "7cd78ca5-269a-4b51-833c-ca2b7315843d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AuthenticationError",
          "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: <your Op*******************************var>. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-8-3719423079.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Example OpenAI Python library request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mMODEL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m response = client.chat.completions.create(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     messages=[\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1085\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m   1086\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1088\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1247\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m         )\n\u001b[0;32m-> 1249\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: <your Op*******************************var>. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
          ]
        }
      ],
      "source": [
        "# Example OpenAI Python library request\n",
        "MODEL = \"gpt-3.5-turbo\"\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Knock knock.\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Who's there?\"},\n",
        "        {\"role\": \"user\", \"content\": \"Orange.\"},\n",
        "    ],\n",
        "    temperature=0,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9w78KIDeWFt4"
      },
      "outputs": [],
      "source": [
        "print(json.dumps(json.loads(response.model_dump_json()), indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_adhpXMZWFt5"
      },
      "source": [
        "البته، این هم توضیح فیلدهای شیء پاسخ به زبان فارسی:\n",
        "\n",
        "* **`id`**: شناسه درخواست\n",
        "* **`choices`**: لیستی از گزینه‌های تکمیل (معمولاً فقط یک گزینه است، مگر اینکه مقدار `n` بزرگتر از ۱ باشد)\n",
        "\n",
        "  * **`finish_reason`**: دلیل توقف مدل در تولید متن (مثلاً `stop` برای پایان طبیعی، یا `length` اگر محدودیت `max_tokens` رسید)\n",
        "  * **`index`**: شماره اندیس گزینه در لیست گزینه‌ها\n",
        "  * **`logprobs`**: اطلاعات احتمال لگاریتمی برای گزینه انتخاب شده\n",
        "  * **`message`**: شیء پیام تولید شده توسط مدل\n",
        "\n",
        "    * **`content`**: متن پیام\n",
        "    * **`role`**: نقش نویسنده پیام (مثلاً assistant یا user)\n",
        "    * **`tool_calls`**: تماس‌هایی که مدل با ابزارها زده، مانند فراخوانی توابع (اگر ابزار داده شده باشد)\n",
        "* **`created`**: زمان ایجاد درخواست (تایم‌استمپ)\n",
        "* **`model`**: نام کامل مدل استفاده شده برای تولید پاسخ\n",
        "* **`object`**: نوع شیء برگشتی (مثل `chat.completion`)\n",
        "* **`system_fingerprint`**: یک شناسه خاص که نشان‌دهنده تنظیمات بک‌اند است که مدل با آن اجرا شده\n",
        "* **`usage`**: تعداد توکن‌های مصرف شده برای تولید پاسخ، شامل توکن‌های ورودی (prompt)، خروجی (completion) و مجموع آنها\n",
        "\n",
        "اگر نیاز داشتی بیشتر توضیح بدم یا نمونه کد هم بخوای، بگو.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRUJ93nDWFt5"
      },
      "source": [
        "فقط پاسخ را با این دستور استخراج کنید:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1AoNjP9WFt6"
      },
      "outputs": [],
      "source": [
        "response.choices[0].message.content\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2H1g0XXWFt6"
      },
      "source": [
        "حتما! منظور این است که حتی کارهایی که مکالمه‌ای نیستند هم می‌توانند با قالب چت انجام شوند، به این شکل که دستور یا درخواست اصلی را در اولین پیام کاربر قرار می‌دهیم.\n",
        "\n",
        "مثلاً اگر بخواهیم از مدل بخواهیم برنامه‌نویسی غیرهمزمان (asynchronous programming) را به سبک دزد دریایی معروف «بلک‌برد» (Blackbeard) توضیح دهد، مکالمه را اینطور تنظیم می‌کنیم:\n",
        "\n",
        "---\n",
        "\n",
        "**User:**\n",
        "\"لطفاً برنامه‌نویسی غیرهمزمان را به سبک دزد دریایی بلک‌برد توضیح بده.\"\n",
        "\n",
        "---\n",
        "\n",
        "مدل بر اساس این پیام، جواب را به شکل یک مکالمه تولید می‌کند اما لحن و سبک آن مطابق درخواست کاربر خواهد بود.\n",
        "\n",
        "این روش به شما امکان می‌دهد هر نوع درخواست و وظیفه‌ای، چه مکالمه‌ای و چه غیرمکالمه‌ای، را در قالب یک گفت‌وگو با مدل بیان کنید.\n",
        "اگر بخواهی، می‌توانم یک مثال کامل JSON یا کد درخواست API هم برات بفرستم.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwc2kvmOWFt7"
      },
      "outputs": [],
      "source": [
        "# example with a system message\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n",
        "    ],\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCXcsfv_WFt7"
      },
      "outputs": [],
      "source": [
        "# example without a system message\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n",
        "    ],\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQSWKRYSWFt8"
      },
      "source": [
        "## ۳. نکات راهنمایی دادن به مدل gpt-3.5-turbo-0301\n",
        "روش‌های بهینه برای راهنمایی مدل‌ها ممکن است بین نسخه‌های مختلف مدل تغییر کند. نکاتی که در ادامه آمده برای مدل gpt-3.5-turbo-0301 کاربرد دارد و ممکن است برای مدل‌های آینده صادق نباشد.`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rkiA198WFt8"
      },
      "source": [
        "### پیام‌های سیستم\n",
        "پیام سیستم می‌تواند برای آماده‌سازی (priming) دستیار با شخصیت‌ها یا رفتارهای متفاوت استفاده شود.\n",
        "\n",
        "توجه داشته باشید که مدل gpt-3.5-turbo-0301 معمولاً به پیام سیستم به اندازه مدل‌های gpt-4-0314 یا gpt-3.5-turbo-0613 توجه نمی‌کند.\n",
        "به همین دلیل، برای gpt-3.5-turbo-0301 توصیه می‌شود که دستورالعمل‌های مهم را در پیام کاربر (user message) قرار دهید.\n",
        "\n",
        "برخی توسعه‌دهندگان با این روش موفق بوده‌اند که پیام سیستم را در طول مکالمه به سمت پایان منتقل کنند تا توجه مدل هنگام طولانی‌تر شدن گفتگو کمتر منحرف شود."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1lM3nSMWFt8"
      },
      "outputs": [],
      "source": [
        "# An example of a system message that primes the assistant to explain concepts in great depth\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a friendly and helpful teaching assistant. You explain concepts in great depth using simple terms, and you give examples to help people learn. At the end of each explanation, you ask a question to check for understanding\"},\n",
        "        {\"role\": \"user\", \"content\": \"Can you explain how fractions work?\"},\n",
        "    ],\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Us_O639WFt8"
      },
      "outputs": [],
      "source": [
        "# An example of a system message that primes the assistant to give brief, to-the-point answers\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a laconic assistant. You reply with brief, to-the-point answers with no elaboration.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Can you explain how fractions work?\"},\n",
        "    ],\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PzHclkRWFt9"
      },
      "source": [
        "آموزش با چند مثال (Few-shot prompting)\n",
        "در بعضی موارد، به جای گفتن مستقیم به مدل آنچه می‌خواهید، راحت‌تر است که با نشان دادن مثال‌هایی به مدل بفهمانید چه انتظاری دارید.\n",
        "\n",
        "یکی از روش‌های نشان دادن خواسته به مدل، استفاده از پیام‌های نمونه جعلی (faked example messages) است.\n",
        "\n",
        "برای مثال:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThoaFIIlWFt9"
      },
      "outputs": [],
      "source": [
        "# An example of a faked few-shot conversation to prime the model into translating business jargon to simpler speech\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Help me translate the following corporate jargon into plain English.\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Sure, I'd be happy to!\"},\n",
        "        {\"role\": \"user\", \"content\": \"New synergies will help drive top-line growth.\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Things working well together will increase revenue.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},\n",
        "        {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},\n",
        "    ],\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmqnwhGNWFt-"
      },
      "source": [
        "برای اینکه روشن شود پیام‌های نمونه بخشی از یک گفت‌وگوی واقعی نیستند و مدل نباید به آنها ارجاع دهد، می‌توانید فیلد name در پیام‌های system را به example_user و example_assistant تغییر دهید.\n",
        "\n",
        "با این کار، نمونه چندمثالی (few-shot example) بالا را می‌توان این‌گونه نوشت:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfilYJbEWFt-"
      },
      "outputs": [],
      "source": [
        "# The business jargon translation example, but with example names for the example messages\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\"},\n",
        "        {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"New synergies will help drive top-line growth.\"},\n",
        "        {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Things working well together will increase revenue.\"},\n",
        "        {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},\n",
        "        {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},\n",
        "        {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},\n",
        "    ],\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lO2qERWoWFt-"
      },
      "source": [
        "هر بار که تلاش می‌کنید مکالمه‌ها را مهندسی کنید، همیشه در ابتدا موفق نخواهید بود.\n",
        "\n",
        "اگر تلاش‌های اول شما جواب نداد، نترسید و روش‌های مختلف آماده‌سازی (priming) یا شرطی‌سازی (conditioning) مدل را آزمایش کنید.\n",
        "\n",
        "مثلاً یک توسعه‌دهنده متوجه شد که وقتی پیامی از طرف کاربر به مدل می‌دهد با این مضمون:\n",
        "«کار خیلی خوبی تا الان انجام دادی، این‌ها عالی بودند»\n",
        "دقت پاسخ‌های مدل بهتر می‌شود و کیفیت بالاتری ارائه می‌کند.\n",
        "\n",
        "برای ایده‌های بیشتر در مورد چگونگی افزایش قابلیت اطمینان مدل‌ها، می‌توانید راهنمای ما درباره روش‌هایی برای افزایش قابلیت اطمینان را مطالعه کنید. این راهنما برای مدل‌های غیرمکالمه‌ای نوشته شده اما بسیاری از اصول آن همچنان کاربرد دارند."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHgTwSKeWFt-"
      },
      "source": [
        "## ۴. شمارش توکن‌ها\n",
        "وقتی درخواست خود را ارسال می‌کنید، API پیام‌ها را به دنباله‌ای از توکن‌ها تبدیل می‌کند.\n",
        "\n",
        "تعداد توکن‌های استفاده‌شده روی موارد زیر تاثیر دارد:\n",
        "\n",
        "هزینه درخواست\n",
        "\n",
        "مدت زمانی که برای تولید پاسخ صرف می‌شود\n",
        "\n",
        "زمان قطع شدن پاسخ در صورت رسیدن به حداکثر محدودیت توکن (۴۰۹۶ توکن برای gpt-3.5-turbo و ۸۱۹۲ توکن برای gpt-4)\n",
        "\n",
        "شما می‌توانید از تابع زیر برای شمارش تعداد توکن‌هایی که یک لیست پیام مصرف می‌کند استفاده کنید.\n",
        "\n",
        "توجه داشته باشید که روش دقیق شمارش توکن‌ها در مدل‌های مختلف ممکن است متفاوت باشد. پس نتایج تابع زیر تخمینی است و تضمینی دائمی نیست.\n",
        "\n",
        "به‌خصوص، درخواست‌هایی که از ورودی‌های توابع اختیاری (optional functions input) استفاده می‌کنند، توکن‌های بیشتری نسبت به تخمین‌های پایین مصرف خواهند کرد.\n",
        "\n",
        "برای مطالعه بیشتر درباره شمارش توکن‌ها، به چگونه با tiktoken توکن‌ها را بشماریم مراجعه کنید."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBCn7YiZWFt_"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "\n",
        "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\"):\n",
        "    \"\"\"Return the number of tokens used by a list of messages.\"\"\"\n",
        "    try:\n",
        "        encoding = tiktoken.encoding_for_model(model)\n",
        "    except KeyError:\n",
        "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
        "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    if model in {\n",
        "        \"gpt-3.5-turbo-0613\",\n",
        "        \"gpt-3.5-turbo-16k-0613\",\n",
        "        \"gpt-4-0314\",\n",
        "        \"gpt-4-32k-0314\",\n",
        "        \"gpt-4-0613\",\n",
        "        \"gpt-4-32k-0613\",\n",
        "        }:\n",
        "        tokens_per_message = 3\n",
        "        tokens_per_name = 1\n",
        "    elif model == \"gpt-3.5-turbo-0301\":\n",
        "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
        "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
        "    elif \"gpt-3.5-turbo\" in model:\n",
        "        print(\"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\")\n",
        "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\")\n",
        "    elif \"gpt-4\" in model:\n",
        "        print(\"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\")\n",
        "        return num_tokens_from_messages(messages, model=\"gpt-4-0613\")\n",
        "    else:\n",
        "        raise NotImplementedError(\n",
        "            f\"\"\"num_tokens_from_messages() is not implemented for model {model}.\"\"\"\n",
        "        )\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        num_tokens += tokens_per_message\n",
        "        for key, value in message.items():\n",
        "            num_tokens += len(encoding.encode(value))\n",
        "            if key == \"name\":\n",
        "                num_tokens += tokens_per_name\n",
        "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
        "    return num_tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYT29BrjWFt_"
      },
      "outputs": [],
      "source": [
        "# let's verify the function above matches the OpenAI API response\n",
        "example_messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"name\": \"example_user\",\n",
        "        \"content\": \"New synergies will help drive top-line growth.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"name\": \"example_assistant\",\n",
        "        \"content\": \"Things working well together will increase revenue.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"name\": \"example_user\",\n",
        "        \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"name\": \"example_assistant\",\n",
        "        \"content\": \"Let's talk later when we're less busy about how to do better.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\",\n",
        "    },\n",
        "]\n",
        "\n",
        "for model in [\n",
        "    # \"gpt-3.5-turbo-0301\",\n",
        "    # \"gpt-4-0314\",\n",
        "    # \"gpt-4-0613\",\n",
        "    \"gpt-3.5-turbo-1106\",\n",
        "    \"gpt-3.5-turbo\",\n",
        "    \"gpt-4\",\n",
        "    \"gpt-4-1106-preview\",\n",
        "    ]:\n",
        "    print(model)\n",
        "    # example token count from the function defined above\n",
        "    print(f\"{num_tokens_from_messages(example_messages, model)} prompt tokens counted by num_tokens_from_messages().\")\n",
        "    # example token count from the OpenAI API\n",
        "    response = client.chat.completions.create(model=model,\n",
        "    messages=example_messages,\n",
        "    temperature=0,\n",
        "    max_tokens=1)\n",
        "    token = response.usage.prompt_tokens\n",
        "    print(f'{token} prompt tokens counted by the OpenAI API.')\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Lai0yM4WFt_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "openai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "365536dcbde60510dc9073d6b991cd35db2d9bac356a11f5b64279a5e6708b97"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}